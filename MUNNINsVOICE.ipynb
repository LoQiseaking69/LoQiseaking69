{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoQiseaking69/LoQiseaking69/blob/main/MUNNINsVOICE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PBPx-VJqVjL",
        "outputId": "366ed1f3-8757-45e1-da43-ca0aa0d7fd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Script 1: Data Preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/final_dataset_with_additional_data_combined_tidy.csv')\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Perform tokenization\n",
        "questions = df['Question'].values\n",
        "answers = df['Answer'].values\n",
        "\n",
        "input_ids, attention_masks, token_type_ids = [], [], []\n",
        "for q, a in zip(questions, answers):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        q, a,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "    token_type_ids.append(encoded_dict['token_type_ids'])\n",
        "\n",
        "# Calculate start_positions and end_positions for answers\n",
        "start_positions, end_positions = [], []\n",
        "for i, (input_id, answer) in enumerate(zip(input_ids, answers)):\n",
        "    answer_tokens = tokenizer.tokenize(answer)\n",
        "    answer_ids = tokenizer.convert_tokens_to_ids(answer_tokens)\n",
        "\n",
        "    start_idx_tensor = (input_id == answer_ids[0]).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    if len(start_idx_tensor) == 0:  # No match found\n",
        "        start_positions.append(None)\n",
        "        end_positions.append(None)\n",
        "        continue\n",
        "\n",
        "    start_idx = start_idx_tensor[0].item()  # Take the first match\n",
        "    end_idx = start_idx + len(answer_ids) - 1\n",
        "\n",
        "    start_positions.append(start_idx)\n",
        "    end_positions.append(end_idx)\n",
        "\n",
        "# Remove rows where start_positions or end_positions are None\n",
        "df['start_positions'] = start_positions\n",
        "df['end_positions'] = end_positions\n",
        "df.dropna(subset=['start_positions', 'end_positions'], inplace=True)\n",
        "\n",
        "# Add these to DataFrame if you want to save them for later use\n",
        "df['input_ids'] = input_ids\n",
        "df['attention_mask'] = attention_masks\n",
        "df['token_type_ids'] = token_type_ids\n",
        "\n",
        "# You can also save this DataFrame if you'd like\n",
        "df.to_csv('/content/drive/MyDrive/final_dataset_with_positions.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "2roMjAR57lsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script 2: DataLoader Creation\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "\n",
        "# Convert lists of tensors to single tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "\n",
        "# Prepare DataLoader\n",
        "dataset = TensorDataset(input_ids, attention_masks, token_type_ids)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
        "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n",
        "\n",
        "# Convert start_positions and end_positions to tensors\n",
        "start_positions = torch.tensor(df['start_positions'].tolist())\n",
        "end_positions = torch.tensor(df['end_positions'].tolist())\n",
        "\n",
        "# Update the DataLoader\n",
        "dataset = TensorDataset(input_ids, attention_masks, token_type_ids, start_positions, end_positions)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
        "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n"
      ],
      "metadata": {
        "id": "ZGZDKG5s7pgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script 3: Model Training with Checkpoint Saving\n",
        "\n",
        "from transformers import BertForQuestionAnswering\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "# Initialize the BERT model for Question Answering\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Define number of epochs\n",
        "n_epochs = 3\n",
        "\n",
        "# Training loop with checkpoint saving\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        b_input_ids, b_attention_mask, b_token_type_ids, b_start_positions, b_end_positions = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask, token_type_ids=b_token_type_ids,\n",
        "                        start_positions=b_start_positions, end_positions=b_end_positions)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Save a checkpoint at the end of each epoch\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, f\"checkpoint_epoch_{epoch}.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDlsmYq69Keq",
        "outputId": "70fb63be-77fa-4d45-b5f1-7344c70b8d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Script 4: Model Interaction\n",
        "\n",
        "# Interact with the model\n",
        "while True:\n",
        "    question = input(\"Enter your question: \")\n",
        "    encoded_question = tokenizer.encode_plus(\n",
        "        question,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        output = model(encoded_question['input_ids'], attention_mask=encoded_question['attention_mask'], token_type_ids=encoded_question['token_type_ids'])\n",
        "    answer_start = torch.argmax(output.start_logits)\n",
        "    answer_end = torch.argmax(output.end_logits)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded_question['input_ids'][0])\n",
        "    answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "    print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "id": "JthFuwa2719t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Osm-1SpkUA0o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}